{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9417 19T2  Homework 2: Applying and Implementing Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Mon Jul 29 09:18:30 AEST 2019_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this homework is to enable you to:\n",
    "\n",
    "- **apply** parameter search for machine learning algorithms implemented in the Python [scikit-learn](http://scikit-learn.org/stable/index.html) machine learning library\n",
    "- answer questions based on your **analysis** and **interpretation** of the empirical results of such applications, using your knowledge of machine learning\n",
    "- **complete** an implementation of a different version of a learning algorithm you have previously seen\n",
    "\n",
    "After completing this homework you will be able to:\n",
    "\n",
    "- set up a simple grid search over different hyper-parameter settings based on $k$-fold cross-validation to obtain  performance measures on different datasets\n",
    "- compare the performance measures of different algorithm settings \n",
    "- propose properties of algorithms and their hyper-parameters, or datasets, which\n",
    "  may lead to performance differences being observed\n",
    "- suggest reasons for actual observed performance differences in terms of\n",
    "  properties of algorithms, parameter settings or datasets.\n",
    "- read and understand incomplete code for a learning algorithm to the point of being able to complete the implementation and run it successfully on a dataset.\n",
    "\n",
    "There are a total of *10 marks* available.\n",
    "Each homework mark is worth *0.5 course mark*, i.e., homework marks will be scaled\n",
    "to a **course mark out of 5** to contribute to the course total.\n",
    "\n",
    "Deadline: 17:59:59, Monday August  5, 2019.\n",
    "\n",
    "Submission will be via the CSE *give* system (see below).\n",
    "\n",
    "Late penalties: one mark will be deducted from the total for each day late, up to a total of five days. If six or more days late, no marks will be given.\n",
    "\n",
    "Recall the guidance regarding plagiarism in the course introduction: this applies to this homework and if evidence of plagiarism is detected it may result in penalties ranging from loss of marks to suspension.\n",
    "\n",
    "### Format of the questions\n",
    "\n",
    "There are 2 questions in this homework. Question 1 requires answering some multiple-choice questions in the file [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/19T2/hw2/answers.txt). Both questions require you to copy and paste text into the file [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/19T2/hw2/answers.txt). This file **MUST CONTAIN ONLY PLAIN TEXT WITH NO SPECIAL CHARACTERS**.\n",
    "\n",
    "This file will form your submission.\n",
    "\n",
    "In summary, your submission will comprise a single file which should be named as follows:\n",
    "```\n",
    "answers.txt\n",
    "```\n",
    "Please note: files in any format other than plain text **cannot be accepted**.\n",
    "\n",
    "Submit your files using ```give```. On a CSE Linux machine, type the following on the command-line:\n",
    "```\n",
    "$ give cs9417 hw2 answers.txt\n",
    "```\n",
    "\n",
    "Alternatively, you can submit using the web-based interface to ```give```.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "You can download the datasets required for the homework [here](http://www.cse.unsw.edu.au/~cs9417/hw2/datasets.zip).\n",
    "Note: you will need to ensure the dataset files are in the same directory from which you are running this notebook.\n",
    "\n",
    "**Please Note**: this homework uses some datasets in the Attribute-Relation File Format (.arff). To load datasets from '.arff' formatted files, you will need to have installed the ```liac-arff``` package. You can do this using ```pip``` at the command-line, as follows:\n",
    "\n",
    "```\n",
    "$ pip install liac-arff\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 – Overfitting avoidance [Total: 3 marks]\n",
    "\n",
    "Dealing with noisy data is a key issue in machine learning. Unfortunately, even algorithms that have noise-handling mechanisms built-in, like decision trees, can overfit noisy data, unless their \"overfitting avoidance\" or *regularization* hyper-parameters are set properly.\n",
    "\n",
    "You will be using datasets that have had various amounts of \"class noise\" added\n",
    "by randomly changing the actual class value to a different one for a\n",
    "specified percentage of the training data.\n",
    "Here we will specify three arbitrarily chosen levels of noise: low\n",
    "($20\\%$), medium ($50\\%$) and high ($80\\%$).\n",
    "The learning algorithm must try to \"see through\" this noise and learn\n",
    "the best model it can, which is then evaluated on test data *without*\n",
    "added noise to evaluate how well it has avoided fitting the noise.\n",
    "\n",
    "We will also let the algorithm do a limited _grid search_ using cross-validation\n",
    "for the best *over-fitting avoidance* parameter settings on each training set.\n",
    "\n",
    "### Running the classifiers\n",
    "\n",
    "**1(a). [1 mark]** \n",
    "\n",
    "Run the code section in the notebook cells below. This will generate a table of results, which you should copy and paste **WITHOUT MODIFICATION** into the file [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/19T2/hw2/answers.txt)\n",
    "as your answer for \"Question 1(a)\". \n",
    "\n",
    "The output of the code section is a table, which represents the percentage accuracy of classification for the decision tree algorithm. The first column contains the result of the \"Default\" classifier, which is the decision tree algorithm with default parameter settings running on each of the datasets which have had $50\\%$ noise added. From the second column on, in each column the results are obtained by running the decision tree algorithm on $0\\%$, $20\\%$, $50\\%$ and $80\\%$ noise added to each of the datasets, and in the parentheses is shown the result of a [grid search](http://en.wikipedia.org/wiki/Hyperparameter_optimization) that has been applied to determine the best value for a basic parameter of the decision tree algorithm, namely [min_samples_leaf](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) i.e., the minimum number of examples that can be used to make a prediction in the tree, on that dataset. \n",
    "\n",
    "### Result interpretation\n",
    "Answer these questions in the file called [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/19T2/hw2/answers.txt). Your answers must be based on the results table you saved in \"Question 1(a)\".\n",
    "\n",
    "**1(b). [1 mark]** Refer to [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/19T2/hw2/answers.txt).\n",
    "\n",
    "**1(c). [1 mark]** Refer to [*answers.txt*](http://www.cse.unsw.edu.au/~cs9417/19T2/hw2/answers.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for question 1\n",
    "\n",
    "It is only necessary to run the following code to answer the question, but you should also go through it to make sure you know what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for question 1\n",
    "\n",
    "import arff, numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import tree\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixed random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "\n",
    "def label_enc(labels):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(labels)\n",
    "    return le\n",
    "\n",
    "def features_encoders(features,categorical_features='all'):\n",
    "    n_samples, n_features = features.shape\n",
    "    label_encoders = [preprocessing.LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "    X_int = np.zeros_like(features, dtype=np.int)\n",
    "\n",
    "    for i in range(n_features):\n",
    "        feature_i = features[:, i]\n",
    "        label_encoders[i].fit(feature_i)\n",
    "        X_int[:, i] = label_encoders[i].transform(feature_i)\n",
    "        \n",
    "    enc = preprocessing.OneHotEncoder(categorical_features=categorical_features)\n",
    "    return enc.fit(X_int),label_encoders\n",
    "\n",
    "def feature_transform(features,label_encoders, one_hot_encoder):\n",
    "    n_samples, n_features = features.shape\n",
    "    X_int = np.zeros_like(features, dtype=np.int)\n",
    "    \n",
    "    for i in range(n_features):\n",
    "        feature_i = features[:, i]\n",
    "        X_int[:, i] = label_encoders[i].transform(feature_i)\n",
    "\n",
    "    return one_hot_encoder.transform(X_int).toarray()\n",
    "\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    dataset = arff.load(open(path, 'r'))\n",
    "    data = np.array(dataset['data'])\n",
    "    data = pd.DataFrame(data)\n",
    "    data = DataFrameImputer().fit_transform(data).values\n",
    "    attr = dataset['attributes']\n",
    "\n",
    "    # mask categorical features\n",
    "    masks = []\n",
    "    for i in range(len(attr)-1):\n",
    "        if attr[i][1] != 'REAL':\n",
    "            masks.append(i)\n",
    "    return data, masks\n",
    "\n",
    "def preprocess(data,masks, noise_ratio):\n",
    "    # split data\n",
    "    train_data, test_data = train_test_split(data,test_size=0.3,random_state=0)\n",
    "\n",
    "    # test data\n",
    "    test_features = test_data[:,0:test_data.shape[1]-1]\n",
    "    test_labels = test_data[:,test_data.shape[1]-1]\n",
    "\n",
    "    # training data\n",
    "    features = train_data[:,0:train_data.shape[1]-1]\n",
    "    labels = train_data[:,train_data.shape[1]-1]\n",
    "\n",
    "    classes = list(set(labels))\n",
    "    # categorical features need to be encoded\n",
    "    if len(masks):\n",
    "        one_hot_enc, label_encs = features_encoders(data[:,0:data.shape[1]-1],masks)\n",
    "        test_features = feature_transform(test_features,label_encs,one_hot_enc)\n",
    "        features = feature_transform(features,label_encs,one_hot_enc)\n",
    "\n",
    "    le = label_enc(data[:,data.shape[1]-1])\n",
    "    labels = le.transform(train_data[:,train_data.shape[1]-1])\n",
    "    test_labels = le.transform(test_data[:,test_data.shape[1]-1])\n",
    "    \n",
    "    # add noise\n",
    "    np.random.seed(1234)\n",
    "    noise = np.random.randint(len(classes)-1, size=int(len(labels)*noise_ratio))+1\n",
    "    \n",
    "    noise = np.concatenate((noise,np.zeros(len(labels) - len(noise),dtype=np.int)))\n",
    "    labels = (labels + noise) % len(classes)\n",
    "\n",
    "    return features,labels,test_features,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Decision Tree Results                                              \n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "    Dataset     |     Default      |        0%        |       20%        |       50%        |       80%        |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "balance-scale   |  36.70% ( 2)     |  76.06% ( 2)     |  71.28% (12)     |  65.43% (27)     |  18.09% (27)     |\n",
      "\n",
      "primary-tumor   |  25.49% ( 2)     |  37.25% (12)     |  42.16% (12)     |  43.14% (12)     |  26.47% ( 7)     |\n",
      "\n",
      "glass           |  44.62% ( 2)     |  69.23% ( 7)     |  66.15% (22)     |  35.38% (17)     |  29.23% (17)     |\n",
      "\n",
      "heart-h         |  35.96% ( 2)     |  67.42% ( 7)     |  78.65% (22)     |  56.18% (17)     |  20.22% (27)     |\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "paths = ['balance-scale','primary-tumor',\n",
    "         'glass','heart-h']\n",
    "noise = [0,0.2,0.5,0.8]\n",
    "\n",
    "scores = []\n",
    "params = []\n",
    "\n",
    "for path in paths:\n",
    "    score = []\n",
    "    param = []\n",
    "    path += '.arff'\n",
    "    data, masks = load_data(path)\n",
    "    \n",
    "    # training on data with 50% noise and default parameters\n",
    "    features, labels, test_features, test_labels = preprocess(data, masks, 0.5)\n",
    "    tree = DecisionTreeClassifier(random_state=0,min_samples_leaf=2, min_impurity_decrease=0)\n",
    "    tree.fit(features, labels)\n",
    "    tree_preds = tree.predict(test_features)\n",
    "    tree_performance = accuracy_score(test_labels, tree_preds)\n",
    "    score.append(tree_performance)\n",
    "    param.append(tree.get_params()['min_samples_leaf'])\n",
    "    \n",
    "    # training on data with noise levels of 0%, 20%, 50% and 80%\n",
    "    for noise_ratio in noise:\n",
    "        features, labels, test_features, test_labels = preprocess(data, masks, noise_ratio)\n",
    "        param_grid = {'min_samples_leaf': np.arange(2,30,5)}\n",
    "\n",
    "        grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid,cv=10,return_train_score=True)\n",
    "        grid_tree.fit(features, labels)\n",
    "\n",
    "        estimator = grid_tree.best_estimator_\n",
    "        tree_preds = grid_tree.predict(test_features)\n",
    "        tree_performance = accuracy_score(test_labels, tree_preds)\n",
    "        score.append(tree_performance)\n",
    "        param.append(estimator.get_params()['min_samples_leaf'])\n",
    "\n",
    "    scores.append(score)\n",
    "    params.append(param)\n",
    "\n",
    "# print the results\n",
    "header = \"{:^112}\".format(\"Decision Tree Results\") + '\\n' + '-' * 112  + '\\n' + \\\n",
    "\"{:^15} | {:^16} | {:^16} | {:^16} | {:^16} | {:^16} |\".format(\"Dataset\", \"Default\", \"0%\", \"20%\", \"50%\", \"80%\") + \\\n",
    " '\\n' + '-' * 112  + '\\n'\n",
    "\n",
    "# print result table\n",
    "print(header)\n",
    "for i in range(len(scores)):\n",
    "    #scores = score_list[i][1]\n",
    "    print(\"{:<16}\".format(paths[i]),end=\"\")\n",
    "    for j in range(len(params[i])):\n",
    "        print(\"|  {:>6.2%} ({:>2})     \" .format(scores[i][j],params[i][j]),end=\"\")\n",
    "    print('|\\n')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(2,40,5)\n",
    "# help(np.arange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 – Implementation of a simple RNN [Total: 7 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, you will implement a simple recurrent neural network (RNN).\n",
    "\n",
    "Recurrent neural networks are commonly used when the input data has temporal dependencies among consecutive observations, for example time series and text data. With such data, having knowledge of the previous data points in addition to the current helps in prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "RNNs are suitable in such scenarios because they keep a state derived from all previously seen data, which in combination with the current input is used to predict the output.\n",
    "\n",
    "In general, recurrent neural networks work like the following:\n",
    "\n",
    "![Screen%20Shot%202019-07-23%20at%2010.51.21%20am.png](files/screenshots/rnn.png)\n",
    "_(Image credit: Goodfellow, Bengio & Courville (2015) - Deep Learning)_\n",
    "\n",
    "Here, $x$ is the input, and $h$ is the hidden state maintained by the RNN. For each input in the sequence, the RNN takes both the previous state $h_{t-1}$ and the current input $x_t$ to do the prediction.\n",
    "\n",
    "Notice there is only one set of weights in the RNN, but this set of weights is used for the whole sequence of input. In effect, the RNN is chained with itself a number of times equalling the length of the input.\n",
    "\n",
    "Thus for the purpose of training the RNN, a common practice is to unfold the computational graph, and run the standard back-propagation thereon. This technique is also known as back-propagation through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task\n",
    "\n",
    "Given a dataset of partial words (words without the last character), your task is to implement an RNN to predict the last character in the word. Specifically, your RNN will have the first 9 characters of a word as its input, and you need to predict the 10th character. If there are fewer than 10 characters in a word, spaces are used to pad it.\n",
    "\n",
    "Most of the code needed is provided below, what you need to do is to implement the back-propagation through time section in ```NeuralNetwork.fit()```.\n",
    "\n",
    "There are four sections marked ```TO DO: ``` where you need to add your own code to complete a working implementation.\n",
    "\n",
    "**HINT:** review the implementation of the ```backpropagate``` method of the ```NeuralNetwork``` class in the code in the notebook for Lab6 on \"Neural Learning\". That should give you a starting point for your implementation.\n",
    "\n",
    "Ensure that you have the following files you need for training and testing in the directory in which you run this notebook:\n",
    "```\n",
    "training_input.txt\n",
    "training_label.txt\n",
    "testing_input.txt\n",
    "testing_label.txt\n",
    "```\n",
    "\n",
    "**HINT:** if your implementation is correct your output should look something like the following:\n",
    "\n",
    "![Screen%20Shot%202019-07-28%20at%205.24.11%20pm.png](files/screenshots/testing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Submit a text file ```RNN_solutions.txt```, containing only the four sections (together with the comments)\n",
    "\n",
    "Sample submission:\n",
    "\n",
    "```\n",
    "# setup for the current step\n",
    "layer_input = []\n",
    "weight = []\n",
    "\n",
    "# calculate gradients\n",
    "gradients, dW, db = [], [], []\n",
    "\n",
    "# update weights\n",
    "self.weights[0] += 0\n",
    "self.biases[0] += 0\n",
    "\n",
    "# setup for the next step\n",
    "previous_gradients = []\n",
    "layer_output = []\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking\n",
    "\n",
    "If your implementation runs and obtains a testing accuracy of more than 0.5 then your submission will be given full marks.\n",
    "\n",
    "Otherwise, each submitted correct section of your code will receive some part of the total marks, as follows:\n",
    "\n",
    "```\n",
    "# setup for the current step [2 marks]\n",
    "layer_input = []\n",
    "weight = []\n",
    "\n",
    "# calculate gradients [1 mark]\n",
    "gradients, dW, db = [], [], []\n",
    "\n",
    "# update weights [2 marks]\n",
    "self.weights[0] += 0\n",
    "self.biases[0] += 0\n",
    "\n",
    "# setup for the next step [2 marks]\n",
    "previous_gradients = []\n",
    "layer_output = []\n",
    "```\n",
    "\n",
    "**NOTE:** it is OK to split your code for each section over multiple lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to read data\n",
    "def read_data(file_name, encoding_function, expected_length):\n",
    "    with open(file_name, 'r') as f:\n",
    "        return numpy.array([encoding_function(row) for row in f.read().split('\\n') if len(row) == expected_length])\n",
    "\n",
    "def encode_string(s):\n",
    "    return [one_hot_encode_character(c) for c in s]\n",
    "\n",
    "def one_hot_encode_character(c):\n",
    "    base = [0] * 26\n",
    "    index = ord(c) - ord('a')\n",
    "    if index >= 0 and index <= 25:\n",
    "        base[index] = 1\n",
    "    return base\n",
    "\n",
    "def reverse_one_hot_encode(v):\n",
    "    return chr(numpy.argmax(v) + ord('a')) if max(v) > 0 else ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions used in the neural network\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + numpy.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return (1 - x) * x\n",
    "\n",
    "def argmax(x):\n",
    "    return numpy.argmax(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, learning_rate=2, epochs=5000, input_size=9, hidden_layer_size=64):\n",
    "        # activation function and its derivative to be used in backpropagation\n",
    "        self.activation_function = sigmoid\n",
    "        self.derivative_of_activation_function = sigmoid_derivative\n",
    "        self.map_output_to_prediction = argmax\n",
    "\n",
    "        # parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "\n",
    "        # initialisation\n",
    "        numpy.random.seed(77)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X (2225, 9, 26)\n",
    "        y:  (2225, 26)\n",
    "        \"\"\"\n",
    "        # reset timer\n",
    "        timer_base = time.time()\n",
    "        \n",
    "        # initialise the weights of the NN\n",
    "        input_dim = X.shape[2] + self.hidden_layer_size #26+64 = 90\n",
    "        output_dim = y.shape[1] #26\n",
    "\n",
    "        self.weights, self.biases = [], []\n",
    "\n",
    "        previous_layer_size = input_dim\n",
    "        for current_layer_size in [self.hidden_layer_size, output_dim]: #[64,26]\n",
    "            # random initial weights and zero biases\n",
    "            weights_of_current_layer = numpy.random.randn(previous_layer_size, current_layer_size)\n",
    "            bias_of_current_layer = numpy.zeros((1, current_layer_size))\n",
    "\n",
    "            self.weights.append(weights_of_current_layer)\n",
    "            self.biases.append(bias_of_current_layer)\n",
    "            previous_layer_size = current_layer_size\n",
    "\n",
    "        # train the NN\n",
    "        self.accuracy_log = []\n",
    "\n",
    "        for epoch in range(self.epochs + 1):\n",
    "            outputs = self.forward_propagate(X)\n",
    "#             print(\"outputs: \",outputs)\n",
    "            prediction = outputs.pop()\n",
    "#             print(\"prediction: \",prediction)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                accuracy = self.evaluate(prediction, y)\n",
    "                print(f\"In iteration {epoch}, training accuracy is {accuracy}.\")\n",
    "                self.accuracy_log.append(accuracy)\n",
    "\n",
    "            # first step of back-propagation\n",
    "            dEdz = y - prediction\n",
    "            layer_input = outputs.pop()\n",
    "            layer_output = prediction\n",
    "            print(\"-\"*10)\n",
    "            print(\"dEdz\", dEdz.shape) \n",
    "            print(\"layer_input\",  layer_input.shape)\n",
    "            print(\"layer_output\", layer_output.shape) \n",
    "            print(\"prediction\", prediction.shape) \n",
    "\n",
    "            # calculate gradients\n",
    "            dEds, dW, db = self.derivatives_of_last_layer(dEdz, layer_output, layer_input)\n",
    "            print(\"-\"*10)\n",
    "            print(\"dEds shape: \",dEds.shape)\n",
    "            print(\"dW shape: \",dW.shape)\n",
    "            print(\"db shape: \",db.shape)\n",
    "            print(\"self.weights[1] shape: \",self.weights[1].shape)\n",
    "            print(\"self.biases[1] shape: \",self.biases[1].shape)\n",
    "\n",
    "            \n",
    "            # update weights\n",
    "            self.weights[1] += self.learning_rate / X.shape[0] * dW\n",
    "            self.biases[1] += self.learning_rate / X.shape[0] * db\n",
    "\n",
    "            # setup for the next step\n",
    "            previous_gradients = dEds\n",
    "            layer_output = layer_input\n",
    "\n",
    "            # back-propagation through time (unrolled)\n",
    "            for step in range (self.input_size - 1, -1, -1):\n",
    "                # TO DO: setup for the current step\n",
    "                layer_input = outputs.pop()\n",
    "                weight = self.weights[0]\n",
    "                print(\"-\"*10,\"INSIDE BACKwaRD LOOP\")\n",
    "                print(\"layer_input CURReNT: \", layer_input.shape)\n",
    "                # TO DO: calculate gradients\n",
    "#                 gradients, dW, db = [],[],[]\n",
    "                gradients, dW, db =  self.derivatives_of_hidden_layer(layer_output-layer_input,numpy.concatenate((layer_output,prediction), axis=1), layer_input, weight)\n",
    "                print(\"-\"*10)\n",
    "                print(\"gradients shape: \",gradients.shape)\n",
    "                print(\"dW shape: \",dW.shape)\n",
    "                print(\"db shape: \",db.shape)\n",
    "                print(\"self.weights[0] shape: \",self.weights[0].shape)\n",
    "                print(\"self.biases[0] shape: \",self.biases[0].shape)\n",
    "                # TO DO: update weights\n",
    "                self.weights[0] += self.learning_rate * dW.T\n",
    "                self.biases[0] += self.learning_rate * db.reshape(1,64)\n",
    "\n",
    "                # TO DO: setup for the next step\n",
    "                previous_gradients = gradients\n",
    "                layer_output = numpy.matmul(self.weights[0]*input_layer)+self.biases\n",
    "\n",
    "        print(f\"Finished training in {time.time() - timer_base} seconds\")\n",
    "\n",
    "    def test(self, X, y, verbose=True):\n",
    "        predictions = self.forward_propagate(X)[-1]\n",
    "\n",
    "        if verbose:\n",
    "            for index in range(len(predictions)):\n",
    "                prefix = ''.join(reverse_one_hot_encode(v) for v in X[index])\n",
    "                print(f\"Expected {prefix + reverse_one_hot_encode(y[index])}, predicted {prefix + reverse_one_hot_encode(predictions[index])}\")\n",
    "\n",
    "        print(f\"Testing accuracy: {self.evaluate(predictions, y)}\")\n",
    "\n",
    "    def evaluate(self, predictions, target_values):\n",
    "        successful_predictions = numpy.where(self.map_output_to_prediction(predictions) == self.map_output_to_prediction(target_values))\n",
    "        return successful_predictions[0].shape[0] / len(predictions) if successful_predictions else 0\n",
    "\n",
    "\n",
    "    def forward_propagate(self, X):\n",
    "        # initial states\n",
    "        current_state = numpy.zeros((X.shape[0], self.hidden_layer_size))\n",
    "        print(\"-\"*10)\n",
    "        print(\"current_state\", current_state.shape)\n",
    "        outputs = [current_state]\n",
    "        \n",
    "        # forward propagation through time (unrolled)\n",
    "        for step in range(self.input_size):\n",
    "            x = numpy.concatenate((current_state, X[:, step, :]), axis=1)\n",
    "            current_state = self.apply_neuron(self.weights[0], self.biases[0], x)\n",
    "            outputs.append(current_state)\n",
    "        print(\"current_state\", current_state.shape)\n",
    "        # the last layer\n",
    "        output = self.apply_neuron(self.weights[1], self.biases[1], current_state)\n",
    "        outputs.append(output)\n",
    "        return outputs\n",
    "\n",
    "    def apply_neuron(self, w, b, x):\n",
    "        return self.activation_function(numpy.dot(x, w) + b)\n",
    "\n",
    "    def derivatives_of_last_layer(self, dEdz, layer_output, layer_input):\n",
    "        dEds = self.derivative_of_activation_function(layer_output) * dEdz\n",
    "        dW = numpy.dot(layer_input.T, dEds)\n",
    "        db = numpy.sum(dEds, axis=0, keepdims=True)\n",
    "        return dEds, dW, db\n",
    "\n",
    "    def derivatives_of_hidden_layer(self, layer_difference, layer_output, layer_input, weight):\n",
    "#         print(self.derivative_of_activation_function(layer_output).shape())\n",
    "#         print(\"wT: \",(weight.T).shape)\n",
    "#         print(\"layer_difference: \", layer_difference)\n",
    "\n",
    "        a = self.derivative_of_activation_function(layer_output)\n",
    "        b = numpy.dot(layer_difference, weight.T)\n",
    "#         print(\"-\"*10)\n",
    "#         print(\"layer_output\", a.shape)\n",
    "#         print(\"dot :\",b.shape)\n",
    "#         print(\"-\"*10)\n",
    "        gradients = a * b\n",
    "        dW = numpy.dot(layer_input.T, gradients)\n",
    "        db = numpy.sum(gradients, axis=0, keepdims=True)\n",
    "        return gradients, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "current_state (2225, 64)\n",
      "current_state (2225, 64)\n",
      "In iteration 0, training accuracy is 0.005393258426966292.\n",
      "----------\n",
      "dEdz (2225, 26)\n",
      "layer_input (2225, 64)\n",
      "layer_output (2225, 26)\n",
      "prediction (2225, 26)\n",
      "----------\n",
      "dEds shape:  (2225, 26)\n",
      "dW shape:  (64, 26)\n",
      "db shape:  (1, 26)\n",
      "self.weights[1] shape:  (64, 26)\n",
      "self.biases[1] shape:  (1, 26)\n",
      "---------- INSIDE BACKwaRD LOOP\n",
      "layer_input CURReNT:  (2225, 64)\n",
      "----------\n",
      "gradients shape:  (2225, 90)\n",
      "dW shape:  (64, 90)\n",
      "db shape:  (1, 90)\n",
      "self.weights[0] shape:  (90, 64)\n",
      "self.biases[0] shape:  (1, 64)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 90 into shape (1,64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-72dfaae4a195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"inp shape\",training_input.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(\"lbl shape: \",training_label.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-118-c329bdf78cbb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;31m# TO DO: update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;31m# TO DO: setup for the next step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 90 into shape (1,64)"
     ]
    }
   ],
   "source": [
    "training_input = read_data(\"training_input.txt\", encode_string, 9)\n",
    "training_label = read_data(\"training_label.txt\", one_hot_encode_character, 1)\n",
    "\n",
    "model = NeuralNetwork()\n",
    "# print(\"inp shape\",training_input.shape)\n",
    "# print(\"lbl shape: \",training_label.shape)\n",
    "model.fit(training_input, training_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-83edab81d45e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtesting_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"testing_label.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot_encode_character\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(testing_input.shape,testing_label.shape )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "testing_input = read_data(\"testing_input.txt\", encode_string, 9)\n",
    "testing_label = read_data(\"testing_label.txt\", one_hot_encode_character, 1)\n",
    "# print(testing_input.shape,testing_label.shape )\n",
    "model.test(testing_input, testing_label, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial acc: 0.39"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
